{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import scipy.io\n",
    "from scipy.signal import spectrogram\n",
    "import os\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import math\n",
    "from librosa.feature import melspectrogram\n",
    "from librosa.display import specshow\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with 1D convolution\n",
    "\n",
    "# Get dataset\n",
    "class_dict = {'siren': 0,\n",
    "             'jackhammer': 1,\n",
    "             'air_conditioner': 2,\n",
    "             'drilling': 3,\n",
    "             'children_playing': 4,\n",
    "             'street_music': 5,\n",
    "             'dog_bark': 6,\n",
    "             'engine_idling': 7,\n",
    "             'gun_shot': 8,\n",
    "             'car_horn': 9}\n",
    "\n",
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.sample_list = []\n",
    "        \n",
    "        for filename in os.listdir(data_dir):\n",
    "            data, samplerate = librosa.load(f'{data_dir}/{filename}')\n",
    "            label = int(filename.split('-')[0][-1])\n",
    "            self.sample_list.append((label, torch.tensor(data), filename))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sample_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data (audio)\n",
    "trainset = SoundDataset('../Datasets/train/Raw')\n",
    "valset = SoundDataset('../Datasets/val/Raw')\n",
    "testset = SoundDataset('../Datasets/test/Raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Neural Network Model\n",
    "class AudioNet(nn.Module):  #1D convolutions\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=23, stride=5, padding=0) #try making kernels larger, fewer layers, less maxp\n",
    "        self.batchnorm1 = nn.BatchNorm1d(16)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=7, stride=3, padding=0)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(32)\n",
    "        self.lrelu = nn.LeakyReLU()\n",
    "        self.max2 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=5, stride=3, padding=0)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.max3 = nn.MaxPool1d(kernel_size=3)\n",
    "        self.conv4 = nn.Conv1d(64, 64, kernel_size=5, stride=3, padding=0)\n",
    "        self.batchnorm4 = nn.BatchNorm1d(64)\n",
    "        self.conv5 = nn.Conv1d(64, 64, kernel_size=7, stride=3, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 22, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.lrelu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.max2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.drop1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.max3(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batchnorm4(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv5(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        #print(x.shape)\n",
    "        out = self.fc2(x)\n",
    "        #print(out.shape)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "class AudioNetSpect(nn.Module):  #2D convolutions on spectrograms\n",
    "    def __init__(self, num_classes):\n",
    "        super(AudioNetSpect, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=7, stride=1, padding=0) #try making kernels larger, fewer layers, less maxp\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.lrelu1 = nn.LeakyReLU()\n",
    "        self.max1 = nn.MaxPool2d(kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=0)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.drop2 = nn.Dropout(p=0.5)\n",
    "        self.lrelu2 = nn.LeakyReLU()\n",
    "        self.max2 = nn.MaxPool2d(kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=0)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 13, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.lrelu1(x)\n",
    "        x = self.max1(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.lrelu2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.max2(x)\n",
    "        #print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        #print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        #print(x.shape)\n",
    "        out = self.fc2(x)\n",
    "        #print(out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AudioNet(\n",
       "  (conv1): Conv1d(1, 16, kernel_size=(23,), stride=(5,))\n",
       "  (batchnorm1): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv1d(16, 32, kernel_size=(7,), stride=(3,))\n",
       "  (batchnorm2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lrelu): LeakyReLU(negative_slope=0.01)\n",
       "  (max2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv1d(32, 64, kernel_size=(5,), stride=(3,))\n",
       "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (drop1): Dropout(p=0.5, inplace=False)\n",
       "  (max3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv4): Conv1d(64, 64, kernel_size=(5,), stride=(3,))\n",
       "  (batchnorm4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv5): Conv1d(64, 64, kernel_size=(7,), stride=(3,))\n",
       "  (fc1): Linear(in_features=1408, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = AudioNet(num_classes=9)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset = trainset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(dataset = valset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "test_loader = DataLoader(dataset = testset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 1.8439535913260088\n",
      "Val Loss: 1.9279757867688718 \n",
      "\n",
      "Epoch 3\n",
      "Training Loss: 1.5188240072001582\n",
      "Val Loss: 2.0070491057375204 \n",
      "\n",
      "Epoch 5\n",
      "Training Loss: 1.3413328662298727\n",
      "Val Loss: 2.060727837293044 \n",
      "\n",
      "Epoch 7\n",
      "Training Loss: 1.2941167043602986\n",
      "Val Loss: 1.9427455624808436 \n",
      "\n",
      "Epoch 9\n",
      "Training Loss: 1.1509858542594358\n",
      "Val Loss: 2.265263145060643 \n",
      "\n",
      "Epoch 11\n",
      "Training Loss: 1.0799903476583785\n",
      "Val Loss: 2.034434985855351 \n",
      "\n",
      "Epoch 13\n",
      "Training Loss: 1.1053624533224797\n",
      "Val Loss: 2.2580907370733176 \n",
      "\n",
      "Epoch 15\n",
      "Training Loss: 1.0555038188678632\n",
      "Val Loss: 1.8991042790205583 \n",
      "\n",
      "Epoch 17\n",
      "Training Loss: 1.0577878209127896\n",
      "Val Loss: 2.4970634488955787 \n",
      "\n",
      "Epoch 19\n",
      "Training Loss: 0.9782850457274396\n",
      "Val Loss: 2.2641718180283257 \n",
      "\n",
      "Epoch 21\n",
      "Training Loss: 0.886574650372284\n",
      "Val Loss: 2.567906541020974 \n",
      "\n",
      "Epoch 23\n",
      "Training Loss: 0.9144020026576691\n",
      "Val Loss: 2.564814975080283 \n",
      "\n",
      "Epoch 25\n",
      "Training Loss: 0.853685880916706\n",
      "Val Loss: 2.366185043816981 \n",
      "\n",
      "Epoch 27\n",
      "Training Loss: 0.8641154847607233\n",
      "Val Loss: 2.423210326096286 \n",
      "\n",
      "Epoch 29\n",
      "Training Loss: 0.7693608232598373\n",
      "Val Loss: 3.269379491067451 \n",
      "\n",
      "Epoch 31\n",
      "Training Loss: 0.8652602963257527\n",
      "Val Loss: 2.789217248558998 \n",
      "\n",
      "Epoch 33\n",
      "Training Loss: 0.8528580656950024\n",
      "Val Loss: 2.4149730542431707 \n",
      "\n",
      "Epoch 35\n",
      "Training Loss: 0.8314598667016928\n",
      "Val Loss: 2.899207965835281 \n",
      "\n",
      "Epoch 37\n",
      "Training Loss: 0.793350750769394\n",
      "Val Loss: 2.374203869830007 \n",
      "\n",
      "Epoch 39\n",
      "Training Loss: 0.6956091575000597\n",
      "Val Loss: 2.9173460751771927 \n",
      "\n",
      "Epoch 41\n",
      "Training Loss: 0.7226355449445006\n",
      "Val Loss: 2.6804008503323016 \n",
      "\n",
      "Epoch 43\n",
      "Training Loss: 0.6870612502098083\n",
      "Val Loss: 2.479703689398973 \n",
      "\n",
      "Epoch 45\n",
      "Training Loss: 0.7050478039444357\n",
      "Val Loss: 3.080438502780769 \n",
      "\n",
      "Epoch 47\n",
      "Training Loss: 0.7762625850197198\n",
      "Val Loss: 2.6523850197377414 \n",
      "\n",
      "Epoch 49\n",
      "Training Loss: 0.7172326864539713\n",
      "Val Loss: 2.770022368301516 \n",
      "\n",
      "Epoch 50\n",
      "Training Loss: 0.7191506604785505\n",
      "Val Loss: 3.1122075118448422 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "best_val_loss = math.inf\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    tot_train_loss = 0\n",
    "    tot_train_samples = 0\n",
    "    \n",
    "    # Train Loop\n",
    "    model.train()\n",
    "    for i, (label, data, _) in enumerate(train_loader):\n",
    "        data = data.type(torch.FloatTensor).cuda().unsqueeze(1)\n",
    "        label = label.long().cuda()\n",
    "        \n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        outputs = model(data) # get outputs\n",
    "        train_loss = criterion(outputs, label) # get batch loss\n",
    "        tot_train_loss += train_loss.item() # add batch loss to total loss\n",
    "        tot_train_samples += data.shape[0] # add number of samples to total number of samples\n",
    "        train_loss.backward() # get gradients\n",
    "        optimizer.step() # update parameters\n",
    "    avg_train_loss = tot_train_loss / len(train_loader)\n",
    "    \n",
    "    # Eval Loop\n",
    "    tot_val_loss = 0\n",
    "    tot_val_samples = 0\n",
    "    pred_vec = []\n",
    "    label_vec = []\n",
    "    \n",
    "    model.eval()\n",
    "    for i, (label, data, _) in enumerate(val_loader):\n",
    "        data = data.type(torch.FloatTensor).cuda().unsqueeze(1)\n",
    "        label = label.long().cuda()\n",
    "        \n",
    "        outputs = model(data) # get outputs\n",
    "        val_loss = criterion(outputs, label) # get batch loss\n",
    "        tot_val_loss += val_loss.item() # add batch loss to total loss\n",
    "        tot_val_samples += data.shape[0] # add number of samples to total number of samples\n",
    "        \n",
    "    avg_val_loss = tot_val_loss / len(val_loader)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'models/audio_net.pt')\n",
    "        \n",
    "    # print losses every 5 epochs\n",
    "    if (epoch + 1) % 2 == 1 or (epoch + 1) == num_epochs:\n",
    "        print('Epoch {}'.format(epoch + 1))\n",
    "        print('Training Loss: {}'.format(avg_train_loss))\n",
    "        print('Val Loss: {} \\n'.format(avg_val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.4638369886465714\n",
      "recall: 0.445283359460947\n",
      "f1: 0.4192510988346981\n",
      "acc: 0.43365253077975374\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.61      0.64       170\n",
      "           1       0.40      0.63      0.49       162\n",
      "           2       0.34      0.18      0.24       202\n",
      "           3       0.63      0.51      0.56       179\n",
      "           4       0.34      0.67      0.45       203\n",
      "           5       0.35      0.38      0.36       184\n",
      "           6       0.54      0.11      0.18       119\n",
      "           7       0.51      0.28      0.36       204\n",
      "           8       0.39      0.64      0.49        39\n",
      "\n",
      "    accuracy                           0.43      1462\n",
      "   macro avg       0.46      0.45      0.42      1462\n",
      "weighted avg       0.46      0.43      0.42      1462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "# Load the data (audio)\n",
    "trainset = SoundDataset('../Datasets/train/Raw')\n",
    "valset = SoundDataset('../Datasets/val/Raw')\n",
    "testset = SoundDataset('../Datasets/test/Raw')\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset = trainset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(dataset = valset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "test_loader = DataLoader(dataset = testset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "predictions = []\n",
    "gt = []\n",
    "\n",
    "model = AudioNet(num_classes=9)\n",
    "model.load_state_dict(torch.load('models/audio_net.pt'))\n",
    "model.cuda()\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for i, (label, data, _) in enumerate(test_loader):\n",
    "    data = data.type(torch.FloatTensor).cuda().unsqueeze(1)\n",
    "    label = label.long().cuda()\n",
    "\n",
    "    outputs = model(data) # get outputs\n",
    "\n",
    "    # Predictions\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predictions += list(np.array(predicted.cpu()).ravel())\n",
    "    gt += list(np.array(label.cpu()).ravel())\n",
    "\n",
    "# Print performance results\n",
    "print(f'precision: {precision_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'recall: {recall_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'f1: {f1_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'acc: {accuracy_score(gt, predictions)}')\n",
    "print()\n",
    "print(classification_report(gt, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now evaluate the raw audio model on the noisy raw audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.1826003042068713\n",
      "recall: 0.1438400646632264\n",
      "f1: 0.06976997025924174\n",
      "acc: 0.15937072503419972\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       170\n",
      "           1       0.13      0.15      0.14       162\n",
      "           2       1.00      0.00      0.01       202\n",
      "           3       0.15      0.98      0.26       179\n",
      "           4       0.37      0.16      0.22       203\n",
      "           5       0.00      0.00      0.00       184\n",
      "           6       0.00      0.00      0.00       119\n",
      "           7       0.00      0.00      0.00       204\n",
      "           8       0.00      0.00      0.00        39\n",
      "\n",
      "    accuracy                           0.16      1462\n",
      "   macro avg       0.18      0.14      0.07      1462\n",
      "weighted avg       0.22      0.16      0.08      1462\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jmian\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\jmian\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Load the data (audio)\n",
    "trainset = SoundDataset('../Datasets/train/NoisyRaw')\n",
    "valset = SoundDataset('../Datasets/val/NoisyRaw')\n",
    "testset = SoundDataset('../Datasets/test/NoisyRaw')\n",
    "\n",
    "# Get data loaders\n",
    "train_loader = DataLoader(dataset = trainset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(dataset = valset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "test_loader = DataLoader(dataset = testset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "predictions = []\n",
    "gt = []\n",
    "\n",
    "model = AudioNet(num_classes=9)\n",
    "model.load_state_dict(torch.load('models/audio_net.pt'))\n",
    "model.cuda()\n",
    "\n",
    "model.eval()\n",
    "for i, (label, data, _) in enumerate(test_loader):\n",
    "    data = data.type(torch.FloatTensor).cuda().unsqueeze(1)\n",
    "    label = label.long().cuda()\n",
    "\n",
    "    outputs = model(data) # get outputs\n",
    "    val_loss = criterion(outputs, label) # get batch loss\n",
    "    tot_val_loss += val_loss.item() # add batch loss to total loss\n",
    "    tot_val_samples += data.shape[0] # add number of samples to total number of samples\n",
    "\n",
    "    # Predictions\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predictions += list(np.array(predicted.cpu()).ravel())\n",
    "    gt += list(np.array(label.cpu()).ravel())\n",
    "    \n",
    "# Print performance\n",
    "print(f'precision: {precision_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'recall: {recall_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'f1: {f1_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'acc: {accuracy_score(gt, predictions)}')\n",
    "print()\n",
    "print(classification_report(gt, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now train a model on the Noisy raw audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Training Loss: 1.941097106622613\n",
      "Val Loss: 2.1866784393787384 \n",
      "\n",
      "Epoch 3\n",
      "Training Loss: 1.6412584669348123\n",
      "Val Loss: 2.1670641380807627 \n",
      "\n",
      "Epoch 5\n",
      "Training Loss: 1.4389490018720212\n",
      "Val Loss: 2.2892929911613464 \n",
      "\n",
      "Epoch 7\n",
      "Training Loss: 1.3362101649028668\n",
      "Val Loss: 2.132575697225073 \n",
      "\n",
      "Epoch 9\n",
      "Training Loss: 1.2542747954527538\n",
      "Val Loss: 2.4883524607057157 \n",
      "\n",
      "Epoch 11\n",
      "Training Loss: 1.1896734678226968\n",
      "Val Loss: 2.2489837395108263 \n",
      "\n",
      "Epoch 13\n",
      "Training Loss: 1.0794713449650917\n",
      "Val Loss: 2.268553307522898 \n",
      "\n",
      "Epoch 15\n",
      "Training Loss: 1.0439229393107952\n",
      "Val Loss: 2.3620149856028347 \n",
      "\n",
      "Epoch 17\n",
      "Training Loss: 0.9565788539922864\n",
      "Val Loss: 2.581942242124806 \n",
      "\n",
      "Epoch 19\n",
      "Training Loss: 1.028797712015069\n",
      "Val Loss: 2.4926840015079663 \n",
      "\n",
      "Epoch 21\n",
      "Training Loss: 0.8476414230274225\n",
      "Val Loss: 2.4482350401256396 \n",
      "\n",
      "Epoch 23\n",
      "Training Loss: 0.8995547851790553\n",
      "Val Loss: 3.226463768793189 \n",
      "\n",
      "Epoch 25\n",
      "Training Loss: 0.9616107189137003\n",
      "Val Loss: 3.8544176467086957 \n",
      "\n",
      "Epoch 27\n",
      "Training Loss: 0.8147532140863114\n",
      "Val Loss: 2.6762771697148033 \n",
      "\n",
      "Epoch 29\n",
      "Training Loss: 0.7555187530275704\n",
      "Val Loss: 2.995639865813048 \n",
      "\n",
      "Epoch 31\n",
      "Training Loss: 0.7817661992233732\n",
      "Val Loss: 4.847172316973624 \n",
      "\n",
      "Epoch 33\n",
      "Training Loss: 0.6774988631573186\n",
      "Val Loss: 3.659998567208 \n",
      "\n",
      "Epoch 35\n",
      "Training Loss: 0.7346725640953451\n",
      "Val Loss: 3.566617286723593 \n",
      "\n",
      "Epoch 37\n",
      "Training Loss: 0.8102560574593751\n",
      "Val Loss: 3.1989839387976606 \n",
      "\n",
      "Epoch 39\n",
      "Training Loss: 0.6925234290352766\n",
      "Val Loss: 3.3656205768170566 \n",
      "\n",
      "Epoch 41\n",
      "Training Loss: 0.6398883139093717\n",
      "Val Loss: 3.3903025919976444 \n",
      "\n",
      "Epoch 43\n",
      "Training Loss: 0.7808849198230798\n",
      "Val Loss: 3.3769206119620283 \n",
      "\n",
      "Epoch 45\n",
      "Training Loss: 0.7435768323855988\n",
      "Val Loss: 3.3021986471570055 \n",
      "\n",
      "Epoch 47\n",
      "Training Loss: 0.5878125448589739\n",
      "Val Loss: 3.5135348076405735 \n",
      "\n",
      "Epoch 49\n",
      "Training Loss: 0.629310401669447\n",
      "Val Loss: 3.5574582530104597 \n",
      "\n",
      "Epoch 50\n",
      "Training Loss: 0.6716618872639062\n",
      "Val Loss: 3.2466113282286604 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data (audio)\n",
    "trainset = SoundDataset('../Datasets/train/NoisyRaw')\n",
    "valset = SoundDataset('../Datasets/val/NoisyRaw')\n",
    "testset = SoundDataset('../Datasets/test/NoisyRaw')\n",
    "\n",
    "# Load model\n",
    "model = AudioNet(num_classes=9)\n",
    "model.cuda()\n",
    "\n",
    "# Get loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset = trainset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(dataset = valset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "test_loader = DataLoader(dataset = testset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "best_val_loss = math.inf\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    tot_train_loss = 0\n",
    "    tot_train_samples = 0\n",
    "    \n",
    "    # Train Loop\n",
    "    model.train()\n",
    "    for i, (label, data, _) in enumerate(train_loader):\n",
    "        data = data.type(torch.FloatTensor).cuda().unsqueeze(1)\n",
    "        label = label.long().cuda()\n",
    "        \n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        outputs = model(data) # get outputs\n",
    "        train_loss = criterion(outputs, label) # get batch loss\n",
    "        tot_train_loss += train_loss.item() # add batch loss to total loss\n",
    "        tot_train_samples += data.shape[0] # add number of samples to total number of samples\n",
    "        train_loss.backward() # get gradients\n",
    "        optimizer.step() # update parameters\n",
    "    avg_train_loss = tot_train_loss / len(train_loader)\n",
    "    \n",
    "    # Eval Loop\n",
    "    tot_val_loss = 0\n",
    "    tot_val_samples = 0\n",
    "    pred_vec = []\n",
    "    label_vec = []\n",
    "    \n",
    "    model.eval()\n",
    "    for i, (label, data, _) in enumerate(val_loader):\n",
    "        data = data.type(torch.FloatTensor).cuda().unsqueeze(1)\n",
    "        label = label.long().cuda()\n",
    "        \n",
    "        outputs = model(data) # get outputs\n",
    "        val_loss = criterion(outputs, label) # get batch loss\n",
    "        tot_val_loss += val_loss.item() # add batch loss to total loss\n",
    "        tot_val_samples += data.shape[0] # add number of samples to total number of samples\n",
    "        \n",
    "    avg_val_loss = tot_val_loss / len(val_loader)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'models/audio_net_noisy.pt')\n",
    "        \n",
    "    # print losses every 5 epochs\n",
    "    if (epoch + 1) % 2 == 1 or (epoch + 1) == num_epochs:\n",
    "        print('Epoch {}'.format(epoch + 1))\n",
    "        print('Training Loss: {}'.format(avg_train_loss))\n",
    "        print('Val Loss: {} \\n'.format(avg_val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now evaluate the noisy-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on noisy raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.445975524527047\n",
      "recall: 0.3260443096231014\n",
      "f1: 0.33204761721043996\n",
      "acc: 0.3454172366621067\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.51      0.61       170\n",
      "           1       0.34      0.18      0.23       162\n",
      "           2       0.25      0.15      0.19       202\n",
      "           3       0.37      0.51      0.43       179\n",
      "           4       0.32      0.37      0.34       203\n",
      "           5       0.22      0.58      0.32       184\n",
      "           6       0.50      0.07      0.12       119\n",
      "           7       0.45      0.34      0.38       204\n",
      "           8       0.82      0.23      0.36        39\n",
      "\n",
      "    accuracy                           0.35      1462\n",
      "   macro avg       0.45      0.33      0.33      1462\n",
      "weighted avg       0.40      0.35      0.34      1462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data (audio)\n",
    "trainset = SoundDataset('../Datasets/train/NoisyRaw')\n",
    "valset = SoundDataset('../Datasets/val/NoisyRaw')\n",
    "testset = SoundDataset('../Datasets/test/NoisyRaw')\n",
    "\n",
    "# Get data loaders\n",
    "train_loader = DataLoader(dataset = trainset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(dataset = valset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "test_loader = DataLoader(dataset = testset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "predictions = []\n",
    "gt = []\n",
    "\n",
    "model = AudioNet(num_classes=9)\n",
    "model.load_state_dict(torch.load('models/audio_net_noisy.pt'))\n",
    "model.cuda()\n",
    "\n",
    "model.eval()\n",
    "for i, (label, data, _) in enumerate(test_loader):\n",
    "    data = data.type(torch.FloatTensor).cuda().unsqueeze(1)\n",
    "    label = label.long().cuda()\n",
    "\n",
    "    outputs = model(data) # get outputs\n",
    "    val_loss = criterion(outputs, label) # get batch loss\n",
    "    tot_val_loss += val_loss.item() # add batch loss to total loss\n",
    "    tot_val_samples += data.shape[0] # add number of samples to total number of samples\n",
    "\n",
    "    # Predictions\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predictions += list(np.array(predicted.cpu()).ravel())\n",
    "    gt += list(np.array(label.cpu()).ravel())\n",
    "    \n",
    "# Print performance\n",
    "print(f'precision: {precision_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'recall: {recall_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'f1: {f1_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'acc: {accuracy_score(gt, predictions)}')\n",
    "print()\n",
    "print(classification_report(gt, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on clean raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.4775651011296691\n",
      "recall: 0.2981091038890537\n",
      "f1: 0.29072049488557145\n",
      "acc: 0.32694938440492477\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.64      0.50       170\n",
      "           1       0.54      0.09      0.16       162\n",
      "           2       0.26      0.29      0.27       202\n",
      "           3       0.62      0.26      0.37       179\n",
      "           4       0.32      0.30      0.31       203\n",
      "           5       0.20      0.54      0.29       184\n",
      "           6       1.00      0.05      0.10       119\n",
      "           7       0.46      0.38      0.42       204\n",
      "           8       0.50      0.13      0.20        39\n",
      "\n",
      "    accuracy                           0.33      1462\n",
      "   macro avg       0.48      0.30      0.29      1462\n",
      "weighted avg       0.45      0.33      0.31      1462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data (audio)\n",
    "trainset = SoundDataset('../Datasets/train/Raw')\n",
    "valset = SoundDataset('../Datasets/val/Raw')\n",
    "testset = SoundDataset('../Datasets/test/Raw')\n",
    "\n",
    "# Get data loaders\n",
    "train_loader = DataLoader(dataset = trainset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = True)\n",
    "\n",
    "val_loader = DataLoader(dataset = valset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "test_loader = DataLoader(dataset = testset,\n",
    "                      batch_size = 32,\n",
    "                      shuffle = False)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "predictions = []\n",
    "gt = []\n",
    "\n",
    "model = AudioNet(num_classes=9)\n",
    "model.load_state_dict(torch.load('models/audio_net_noisy.pt'))\n",
    "model.cuda()\n",
    "\n",
    "model.eval()\n",
    "for i, (label, data, _) in enumerate(test_loader):\n",
    "    data = data.type(torch.FloatTensor).cuda().unsqueeze(1)\n",
    "    label = label.long().cuda()\n",
    "\n",
    "    outputs = model(data) # get outputs\n",
    "    val_loss = criterion(outputs, label) # get batch loss\n",
    "    tot_val_loss += val_loss.item() # add batch loss to total loss\n",
    "    tot_val_samples += data.shape[0] # add number of samples to total number of samples\n",
    "\n",
    "    # Predictions\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    predictions += list(np.array(predicted.cpu()).ravel())\n",
    "    gt += list(np.array(label.cpu()).ravel())\n",
    "    \n",
    "# Print performance\n",
    "print(f'precision: {precision_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'recall: {recall_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'f1: {f1_score(gt, predictions, average=\"macro\")}')\n",
    "print(f'acc: {accuracy_score(gt, predictions)}')\n",
    "print()\n",
    "print(classification_report(gt, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Everything below is scratch work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlp]",
   "language": "python",
   "name": "conda-env-nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
